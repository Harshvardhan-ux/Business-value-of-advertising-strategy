{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas to access Data Frame\n",
    "import numpy as np #import Numpy for Data Frame manipulation\n",
    "from bs4 import BeautifulSoup as bs #import beatiful soup for web scrapping\n",
    "import requests #import requests to fetch records\n",
    "\n",
    "Orig_fortune = open(\"fortune1000.txt\",\"r\") # original fortune 1000 text file\n",
    "\n",
    "#Reading text file into list\n",
    "newdata = Orig_fortune.readlines()\n",
    "\n",
    "#Reading records into list\n",
    "rawdatalist = [line.strip() for line in newdata]\n",
    "\n",
    "#converting list into array\n",
    "rawarray = np.array(rawdatalist)\n",
    "\n",
    "#creating dataframe and reshaping it\n",
    "df2 = pd.DataFrame(rawarray.reshape(1000,11), columns = ['Rank' ,'Name','Revenue($m)','%change in Revenue','Profit($m)','% Change in Profits', 'Assets($m)','Market_Value($m)','Change in Rank (1000)', 'Employees', 'Change in Rank (500 Only)'] )\n",
    "\n",
    "#dropping unnecessary data\n",
    "df2.drop(['%change in Revenue','% Change in Profits','Change in Rank (1000)','Change in Rank (500 Only)'],axis=1,inplace =True)\n",
    "\n",
    "#replacing special characters with blank\n",
    "df2['Revenue($m)'] = df2['Revenue($m)'].str.replace('$','')\n",
    "df2['Revenue($m)'] = df2['Revenue($m)'].str.replace(',','')\n",
    "df2['Profit($m)'] = df2['Profit($m)'].str.replace('$','')\n",
    "df2['Profit($m)'] = df2['Profit($m)'].str.replace(',','')\n",
    "df2['Assets($m)'] = df2['Assets($m)'].str.replace('$','')\n",
    "df2['Assets($m)'] = df2['Assets($m)'].str.replace(',','')\n",
    "df2['Market_Value($m)'] = df2['Market_Value($m)'].str.replace('$','')\n",
    "df2['Market_Value($m)'] = df2['Market_Value($m)'].str.replace(',','')\n",
    "df2['Employees'] = df2['Employees'].str.replace(',','')\n",
    "\n",
    "#saving the file to csv\n",
    "df2.to_csv('Fortune1000_TransformData.csv')\n",
    "\n",
    "\n",
    "# Srcapping data\n",
    "#Reading Csv file into dataframe\n",
    "df = pd.read_csv('Fortune1000_TransformData.csv')\n",
    "comp_url = '.websiteoutlook.com'\n",
    "\n",
    "#assigning URL from dataframe to a list variable\n",
    "url_list = df.URL\n",
    "\n",
    "#Appending the url list for analysis\n",
    "update_list = url_list + comp_url\n",
    "\n",
    "#list variables to store data\n",
    "Alexa_rank = []\n",
    "Backlinks = []\n",
    "Page_authority = []\n",
    "Domain_authority = []\n",
    "Moz_Rank = []\n",
    "Page_views = []\n",
    "worth = []\n",
    "Page_size = []\n",
    "code_to_text_ratio = []\n",
    "Sem_rank = []\n",
    "Keyword = []\n",
    "organic_traffic = []\n",
    "cost = []\n",
    "adv_keyword = []\n",
    "adv_traffic= []\n",
    "adv_budget = []\n",
    "\n",
    "#Fetching Records\n",
    "for url in update_list[:20]:   #for 20 records\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.content, \"html.parser\")\n",
    "    Alexa_rank.append(soup.find('span',{\"class\":\"label label-primary\"}).get_text().strip())\n",
    "    Backlinks.append(soup.find('span',{\"class\":\"label label-default\"}).get_text().strip())\n",
    "    Page_authority.append(soup.find('span',{\"class\":\"label label-info\"}).get_text().strip())\n",
    "    Domain_authority.append(soup.find('span',{\"class\":\"label label-info\"}).get_text().strip())\n",
    "    Moz_Rank.append(soup.find('span',{\"class\":\"label label-info\"}).get_text().strip())\n",
    "    Page_views.append(soup.find('span',{\"class\":\"label label-warning\"}).get_text().strip())\n",
    "    worth.append(soup.find('span',{\"class\":\"label label-danger\"}).get_text().strip())\n",
    "    Page_size = soup.find(\"dt\",text=\"Page Size\").findNext(\"dd\").string.strip() #To find the next DD tag and fetch specific data. Most efficient way\n",
    "    code_to_text_ratio = soup.find(\"dt\",text=\"Code to Text Ratio\").findNext(\"dd\").string.strip()\n",
    "    table_data = soup.find(\"table\",{\"class\":\"table table-bordered table-condensed\"}) #Fetching table records\n",
    "    str_words = table_data.find_all(\"td\")\n",
    "    Sem_rank.append((str_words[1]).get_text())\n",
    "    Keyword.append((str_words[4]).get_text())\n",
    "    organic_traffic.append((str_words[7]).get_text())\n",
    "    cost.append((str_words[10]).get_text())\n",
    "    adv_keyword.append((str_words[13]).get_text())\n",
    "    adv_traffic.append((str_words[16]).get_text())\n",
    "    adv_budget.append((str_words[19]).get_text())\n",
    "    \n",
    "#storing list variables into dataframe    \n",
    "df1 = pd.DataFrame(list(zip(Alexa_rank,Backlinks,Page_authority,Domain_authority,Moz_rank,Page_views,worth,Page_size,code_to_text_ratio,Sem_rank,Keyword,organic_traffic,cost,adv_keyword,adv_traffic,adv_budget)),columns = [\"Alexa Rank\",\"Backlinks\",\"Page Authority\",\"Domain Authority\",\"Moz Rank\",\"Page Views\",\"Worth\",\"Page Size\",\"Code to text Ratio%\",\"Sem Rank\",\"Keyword\",\"Organic Traffic\",\"Cost\",\"Adwords Keyword\",\"Adwords Traffic\",\"Adwords Budget\"])    \n",
    "    \n",
    "#saving dataframe into csv file    \n",
    "df1.to_csv('Scrapping_data.csv') \n",
    "\n",
    "# ****IMPORTANT*****\n",
    "\n",
    "#Last Point in pdf ask us to combine the two dataframe which I suppose is logically incorrect because the data inside the second dataframe belongs to the first record in first dataframe which implies it to be a dataframe multidimentional array and such thing can't be stored in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
